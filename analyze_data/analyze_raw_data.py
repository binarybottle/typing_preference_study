"""
Bigram Typing Analysis Script

Analyzes raw Typing Preference Study data to determine relationships 
between bigram typing times, errors, and frequencies.

Usage:
    python analyze_raw_data.py [--config config.yaml]
"""
import numpy as np
import pandas as pd
import os
import glob
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from collections import defaultdict
from scipy import stats

# Define the left home block keys and mirror pairs
LEFT_HOME_KEYS = ['q', 'w', 'e', 'r', 'a', 's', 'd', 'f', 'z', 'x', 'c', 'v']
MIRROR_PAIRS = [
    ('q', 'p'), ('w', 'o'), ('e', 'i'), ('r', 'u'),
    ('a', ';'), ('s', 'l'), ('d', 'k'), ('f', 'j'), ('g', 'h'),
    ('z', '/'), ('x', '.'), ('c', ','), ('v', 'm')
]

#-------------------------------------------------------------------------------
# Load and process data
#-------------------------------------------------------------------------------
def load_frequency_data(letter_freq_path=None, bigram_freq_path=None):
    """
    Load frequency data from CSV files
    Parameters:
    letter_freq_path (str): Path to letter frequency CSV
    bigram_freq_path (str): Path to bigram frequency CSV
    Returns:
    tuple: (letter_frequencies, bigram_frequencies) as dataframes
    """
    letter_frequencies = None
    bigram_frequencies = None
    try:
        if letter_freq_path and os.path.exists(letter_freq_path):
            letter_frequencies = pd.read_csv(letter_freq_path)
            print(f"Loaded letter frequency data: {len(letter_frequencies)} entries")
    except Exception as e:
        print(f"Error loading letter frequency data: {str(e)}")
    try:
        if bigram_freq_path and os.path.exists(bigram_freq_path):
            bigram_frequencies = pd.read_csv(bigram_freq_path)
            print(f"Loaded bigram frequency data: {len(bigram_frequencies)} entries")
    except Exception as e:
        print(f"Error loading bigram frequency data: {str(e)}")
    return letter_frequencies, bigram_frequencies

def process_typing_data(data):
    """Process typing data to calculate typing time between consecutive keystrokes"""
    # Make an explicit copy of the DataFrame to avoid the SettingWithCopyWarning
    data = data.copy()
    # Convert expectedKey and typedKey to string if they're not already
    data.loc[:, 'expectedKey'] = data['expectedKey'].astype(str)
    data.loc[:, 'typedKey'] = data['typedKey'].astype(str)
    # Replace 'nan' strings with empty strings
    data.loc[:, 'expectedKey'] = data['expectedKey'].replace('nan', '')
    data.loc[:, 'typedKey'] = data['typedKey'].replace('nan', '')
    # Ensure isCorrect is boolean
    data.loc[:, 'isCorrect'] = data['isCorrect'].map(lambda x: str(x).lower() == 'true')
    processed = []
    # Sort by user and timestamp to ensure correct sequence
    sorted_data = data.sort_values(by=['user_id', 'keydownTime'])
    # Group by user_id
    for user_id, user_data in sorted_data.groupby('user_id'):
        user_rows = user_data.to_dict('records')
        # Calculate typing time for each keystroke
        for i in range(1, len(user_rows)):
            current = user_rows[i]
            previous = user_rows[i-1]
            # Calculate time difference in milliseconds
            typing_time = current['keydownTime'] - previous['keydownTime']
            # Add processed data
            processed.append({
                'user_id': user_id,
                'trialId': current['trialId'],
                'expectedKey': current['expectedKey'],
                'typedKey': current['typedKey'],
                'isCorrect': current['isCorrect'],
                'typingTime': typing_time,
                'keydownTime': current['keydownTime']
            })
    return pd.DataFrame(processed)

def adjust_keys_for_frequency(key_df, letter_freq):
    """
    Adjust individual key metrics by regressing out letter frequency effects
    
    Parameters:
    key_df (DataFrame): DataFrame with key statistics
    letter_freq (DataFrame): DataFrame with letter frequencies
    
    Returns:
    DataFrame: Key statistics with frequency-adjusted metrics
    """
    # Make a copy to avoid modifying the original
    adjusted_df = key_df.copy()
    
    # Create frequency dictionary
    freq_dict = {}
    for _, row in letter_freq.iterrows():
        letter_value = str(row['item']).lower()
        if len(letter_value) == 1:  # Only use single characters
            freq_dict[letter_value] = row['score']
    
    # Add frequency data to DataFrame
    adjusted_df['frequency'] = adjusted_df['key'].map(freq_dict).fillna(0)
    
    # Add log-transformed frequency (common in psycholinguistic research)
    adjusted_df['log_freq'] = np.log10(adjusted_df['frequency'] + 1)  # Add 1 to handle zeros
    
    # Metrics to adjust
    metrics = ['errorRate', 'medianTime']
    
    # Adjust each metric
    for metric in metrics:
        if metric in adjusted_df.columns:
            try:
                # Create the model: metric ~ log_frequency
                X = sm.add_constant(adjusted_df['log_freq'])
                y = adjusted_df[metric]
                
                # Fit the model
                model = sm.OLS(y, X).fit()
                
                # Get the model parameters
                intercept = model.params[0]
                slope = model.params[1]
                
                # Calculate predicted values
                adjusted_df[f'{metric}_pred'] = intercept + slope * adjusted_df['log_freq']
                
                # Calculate residuals (the frequency-adjusted values) + mean
                adjusted_df[f'{metric}_freq_adjusted'] = adjusted_df[metric] - adjusted_df[f'{metric}_pred'] + adjusted_df[metric].mean()
                
                print(f"Adjusted {metric} for keys (R² = {model.rsquared:.3f})")
            except Exception as e:
                print(f"Error adjusting {metric} for keys: {e}")
    
    return adjusted_df

def adjust_bigrams_for_frequency(bigram_df, bigram_freq):
    """
    Adjust bigram metrics by regressing out bigram frequency effects
    
    Parameters:
    bigram_df (DataFrame): DataFrame with bigram statistics
    bigram_freq (DataFrame): DataFrame with bigram frequencies
    
    Returns:
    DataFrame: Bigram statistics with frequency-adjusted metrics
    """
    # Make a copy to avoid modifying the original
    adjusted_df = bigram_df.copy()
    
    # Create frequency dictionary
    freq_dict = {}
    for _, row in bigram_freq.iterrows():
        bigram_value = str(row['item_pair']).lower()
        if len(bigram_value) == 2:  # Only use bigrams
            freq_dict[bigram_value] = row['score']
    
    # Add frequency data to DataFrame
    adjusted_df['frequency'] = adjusted_df['bigram'].map(freq_dict).fillna(0)
    
    # Add log-transformed frequency
    adjusted_df['log_freq'] = np.log10(adjusted_df['frequency'] + 1)  # Add 1 to handle zeros
    
    # Metrics to adjust
    metrics = ['errorRate', 'medianTime']
    
    # Adjust each metric
    for metric in metrics:
        if metric in adjusted_df.columns:
            try:
                # Filter to only rows with some frequency data
                model_df = adjusted_df[adjusted_df['frequency'] > 0]
                
                if len(model_df) > 5:  # Need enough data points for regression
                    # Create the model: metric ~ log_frequency
                    X = sm.add_constant(model_df['log_freq'])
                    y = model_df[metric]
                    
                    # Fit the model
                    model = sm.OLS(y, X).fit()
                    
                    # Get the model parameters
                    intercept = model.params[0]
                    slope = model.params[1]
                    
                    # Calculate predicted values for all bigrams
                    adjusted_df[f'{metric}_pred'] = intercept + slope * adjusted_df['log_freq']
                    
                    # Calculate residuals (the frequency-adjusted values) + mean
                    adjusted_df[f'{metric}_freq_adjusted'] = adjusted_df[metric] - adjusted_df[f'{metric}_pred'] + adjusted_df[metric].mean()
                    
                    print(f"Adjusted {metric} for bigrams (R² = {model.rsquared:.3f})")
                else:
                    print(f"Not enough data points with frequency to adjust {metric} for bigrams")
            except Exception as e:
                print(f"Error adjusting {metric} for bigrams: {e}")
    
    return adjusted_df

#-------------------------------------------------------------------------------
# Analyze keys
#-------------------------------------------------------------------------------
def analyze_key_stats(data, keys_to_include=None):
    """
    Analyze 1-key and 2-key statistics for typing data.
    
    For 1-key stats: Focus on individual keys in the context of being the second key in any bigram
    For 2-key stats: Focus on specific bigram combinations
    
    All calculations focus on space-flanked bigrams (space-key1-key2-space) and exclude same-key bigrams.
    
    Parameters:
    data (DataFrame): Processed typing data
    keys_to_include (list): List of keys to include in analysis (if None, include all)
    
    Returns:
    tuple: (key_stats, bigram_stats) - Dictionaries for individual key and bigram statistics
    """
    key_stats = {}  # For 1-key statistics
    bigram_stats = {}  # For 2-key statistics
    
    # Group by user_id and trialId
    grouped = data.groupby(['user_id', 'trialId'])
    
    # Process each trial
    for (user_id, trial_id), trial_data in grouped:
        # Sort by timestamp to ensure correct sequence
        sorted_trial = trial_data.sort_values('keydownTime').reset_index(drop=True)
        
        i = 0
        while i < len(sorted_trial) - 3:  # Need at least 4 keystrokes: space, key1, key2, space
            # Get 4 consecutive keystrokes
            first = sorted_trial.iloc[i]     # should be a space
            second = sorted_trial.iloc[i+1]  # key1
            third = sorted_trial.iloc[i+2]   # key2 (key of interest for 1-key stats)
            fourth = sorted_trial.iloc[i+3]  # should be a space
            
            # Check if we have a valid flanked bigram pattern: space, key1, key2, space
            if (isinstance(first['expectedKey'], str) and first['expectedKey'] == ' ' and
                isinstance(second['expectedKey'], str) and len(second['expectedKey']) == 1 and second['expectedKey'] != ' ' and
                isinstance(third['expectedKey'], str) and len(third['expectedKey']) == 1 and third['expectedKey'] != ' ' and
                isinstance(fourth['expectedKey'], str) and fourth['expectedKey'] == ' '):
                
                # Skip same-key bigrams (where key1 == key2)
                if second['expectedKey'] == third['expectedKey']:
                    i += 1
                    continue
                
                key1 = second['expectedKey']
                key2 = third['expectedKey']
                
                # Check if keys are in the specified list
                if keys_to_include:
                    # For 2-key stats, both keys must be in keys_to_include
                    if key1 not in keys_to_include or key2 not in keys_to_include:
                        i += 1
                        continue
                
                # === PROCESS INDIVIDUAL KEY STATS (1-key stats for key2) ===
                
                # Initialize stats for this key if not exists
                if key2 not in key_stats:
                    key_stats[key2] = {
                        'key': key2,
                        'totalCount': 0,
                        'errorCount': 0,
                        'error_types': defaultdict(int),
                        'timeSamples': [],
                        'totalTime': 0
                    }
                
                # Count occurrences for individual key
                key_stats[key2]['totalCount'] += 1
                
                # Check if the key was typed incorrectly
                if not third['isCorrect']:
                    # Count as one error regardless of how many attempts were made
                    key_stats[key2]['errorCount'] += 1
                    
                    # Record the incorrect key that was typed
                    if isinstance(third['typedKey'], str) and third['typedKey']:
                        key_stats[key2]['error_types'][third['typedKey']] += 1
                    
                    # Find all incorrect attempts for this key
                    error_position = i + 2
                    while (error_position < len(sorted_trial) and 
                           sorted_trial.iloc[error_position]['expectedKey'] == key2 and 
                           not sorted_trial.iloc[error_position]['isCorrect']):
                        # Record each incorrect key that was typed
                        typed_key = sorted_trial.iloc[error_position]['typedKey']
                        if isinstance(typed_key, str) and typed_key:
                            key_stats[key2]['error_types'][typed_key] += 1
                        error_position += 1
                elif second['isCorrect']:
                    # Only calculate timing if both keys are correct (no errors)
                    # Use time between key1 and key2
                    key_time = third['keydownTime'] - second['keydownTime']
                    if 50 <= key_time <= 2000:  # Reasonable time range
                        key_stats[key2]['timeSamples'].append(key_time)
                        key_stats[key2]['totalTime'] += key_time
                
                # === PROCESS BIGRAM STATS (2-key stats) ===
                
                # Form the bigram key
                bigram = key1 + key2
                
                # Initialize stats for this bigram if not exists
                if bigram not in bigram_stats:
                    bigram_stats[bigram] = {
                        'bigram': bigram,
                        'totalCount': 0,
                        'errorCount': 0,
                        'error_types': defaultdict(int),
                        'timeSamples': [],
                        'totalTime': 0
                    }
                
                # Count this bigram occurrence
                bigram_stats[bigram]['totalCount'] += 1
                
                # Check if key2 was typed incorrectly while key1 was correct
                if second['isCorrect'] and not third['isCorrect']:
                    # Count as one error regardless of how many attempts were made
                    bigram_stats[bigram]['errorCount'] += 1
                    
                    # Record the incorrect key that was typed
                    if isinstance(third['typedKey'], str) and third['typedKey']:
                        bigram_stats[bigram]['error_types'][third['typedKey']] += 1
                    
                    # Find all incorrect attempts for key2
                    error_position = i + 2
                    while (error_position < len(sorted_trial) and 
                           sorted_trial.iloc[error_position]['expectedKey'] == key2 and 
                           not sorted_trial.iloc[error_position]['isCorrect']):
                        # Record each incorrect key that was typed
                        typed_key = sorted_trial.iloc[error_position]['typedKey']
                        if isinstance(typed_key, str) and typed_key:
                            bigram_stats[bigram]['error_types'][typed_key] += 1
                        error_position += 1
                elif second['isCorrect'] and third['isCorrect']:
                    # Only calculate timing if both keys are correct (no errors)
                    # Use time between key1 and key2
                    bigram_time = third['keydownTime'] - second['keydownTime']
                    if 50 <= bigram_time <= 2000:  # Reasonable time range
                        bigram_stats[bigram]['timeSamples'].append(bigram_time)
                        bigram_stats[bigram]['totalTime'] += bigram_time
                
                # Skip past this bigram
                i += 3  # Move to position after the final space
            else:
                i += 1
    
    # Calculate statistics for individual keys
    for stats in key_stats.values():
        # Error rate
        stats['errorRate'] = stats['errorCount'] / stats['totalCount'] if stats['totalCount'] > 0 else 0
        
        # Timing statistics
        if len(stats['timeSamples']) > 0:
            stats['avgTime'] = stats['totalTime'] / len(stats['timeSamples'])
            stats['medianTime'] = np.median(stats['timeSamples'])
            # Calculate Median Absolute Deviation (MAD)
            deviations = np.abs(np.array(stats['timeSamples']) - stats['medianTime'])
            stats['timeMAD'] = np.median(deviations)
        else:
            stats['avgTime'] = 0
            stats['medianTime'] = 0
            stats['timeMAD'] = 0
        
        # Error MAD using bootstrap
        if stats['totalCount'] > 10:
            bootstrap_errors = []
            n_bootstrap = 1000
            for _ in range(n_bootstrap):
                sample = np.random.choice([0, 1], size=stats['totalCount'],
                                           p=[1-stats['errorRate'], stats['errorRate']])
                bootstrap_errors.append(np.mean(sample))
            stats['errorMAD'] = np.median(np.abs(np.array(bootstrap_errors) - stats['errorRate']))
        else:
            stats['errorMAD'] = 0
        
        # Most common error type
        if stats['error_types']:
            most_common_error = max(stats['error_types'].items(), key=lambda x: x[1])
            stats['most_common_mistype'] = most_common_error[0]
            stats['most_common_mistype_count'] = most_common_error[1]
        else:
            stats['most_common_mistype'] = ""
            stats['most_common_mistype_count'] = 0
    
    # Calculate statistics for bigrams
    for stats in bigram_stats.values():
        # Error rate
        stats['errorRate'] = stats['errorCount'] / stats['totalCount'] if stats['totalCount'] > 0 else 0
        
        # Timing statistics
        if len(stats['timeSamples']) > 0:
            stats['avgTime'] = stats['totalTime'] / len(stats['timeSamples'])
            stats['medianTime'] = np.median(stats['timeSamples'])
            # Calculate Median Absolute Deviation (MAD)
            deviations = np.abs(np.array(stats['timeSamples']) - stats['medianTime'])
            stats['timeMAD'] = np.median(deviations)
        else:
            stats['avgTime'] = 0
            stats['medianTime'] = 0
            stats['timeMAD'] = 0
        
        # Error MAD using bootstrap
        if stats['totalCount'] > 10:
            bootstrap_errors = []
            n_bootstrap = 1000
            for _ in range(n_bootstrap):
                sample = np.random.choice([0, 1], size=stats['totalCount'],
                                           p=[1-stats['errorRate'], stats['errorRate']])
                bootstrap_errors.append(np.mean(sample))
            stats['errorMAD'] = np.median(np.abs(np.array(bootstrap_errors) - stats['errorRate']))
        else:
            stats['errorMAD'] = 0
        
        # Most common error type
        if stats['error_types']:
            most_common_error = max(stats['error_types'].items(), key=lambda x: x[1])
            stats['most_common_mistype'] = most_common_error[0]
            stats['most_common_mistype_count'] = most_common_error[1]
        else:
            stats['most_common_mistype'] = ""
            stats['most_common_mistype_count'] = 0
    
    return key_stats, bigram_stats

def visualize_left_home_key_error_rates(key_df, min_count=10):
    """
    Create a visualization of error rates for individual left home block keys
    Parameters:
    key_df (DataFrame): DataFrame containing individual key statistics
    min_count (int): Minimum count threshold for visualization
    """
    # Filter for sufficient data
    filtered_df = key_df[key_df['totalCount'] > min_count].copy()
    if filtered_df.empty:
        print("No data available for key error rate visualization")
        return
    
    # Create output directory for figures
    output_dir = 'output/figures/'
    os.makedirs(output_dir, exist_ok=True)
    
    # Sort by error rate for visualization
    filtered_df = filtered_df.sort_values('errorRate', ascending=False)
    
    # Create error rate plot with MAD error bars
    plt.figure(figsize=(12, 6))
    
    # Convert keys to uppercase for display
    filtered_df['key_upper'] = filtered_df['key'].str.upper()
    
    # Create the bar plot
    bars = plt.bar(filtered_df['key_upper'], filtered_df['errorRate'])
    
    # Add error bars using error MAD
    plt.errorbar(
        x=range(len(filtered_df)), 
        y=filtered_df['errorRate'],
        yerr=filtered_df['errorMAD'], 
        fmt='none', 
        color='black', 
        capsize=5
    )
    
    plt.title('Error rates for left home block keys')
    plt.xlabel('Key')
    plt.ylabel('Error rate')
    
    # Add percentage labels
    for i, bar in enumerate(bars):
        height = bar.get_height()
        if height > 0.001:  # Only add labels if error rate is non-negligible
            plt.text(bar.get_x() + bar.get_width()/2., height/2, f"{height*100:.1f}%",
                    ha='center', color='white', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}error_1key.png')
    plt.close()

def visualize_left_home_key_typing_times(key_df, min_count=10):
    """
    Create a visualization of median typing times for individual left home block keys
    Parameters:
    key_df (DataFrame): DataFrame containing individual key statistics
    min_count (int): Minimum count threshold for visualization
    """
    # Filter for sufficient data
    filtered_df = key_df[key_df['totalCount'] > min_count].copy()
    if filtered_df.empty:
        print("No data available for key typing time visualization")
        return
    
    # Create output directory for figures
    output_dir = 'output/figures/'
    os.makedirs(output_dir, exist_ok=True)
    
    # Sort by median time for visualization
    filtered_df = filtered_df.sort_values('medianTime', ascending=False)
    
    # Create median time plot with MAD error bars
    plt.figure(figsize=(12, 6))
    
    # Convert keys to uppercase for display
    filtered_df['key_upper'] = filtered_df['key'].str.upper()
    
    # Create the bar plot
    bars = plt.bar(filtered_df['key_upper'], filtered_df['medianTime'])
    
    # Add error bars using time MAD
    plt.errorbar(
        x=range(len(filtered_df)), 
        y=filtered_df['medianTime'],
        yerr=filtered_df['timeMAD'], 
        fmt='none', 
        color='black', 
        capsize=5
    )
    
    plt.title('Typing times for left home block keys')
    plt.xlabel('Key')
    plt.ylabel('Median typing time (ms)')
    
    # Add time labels
    for i, bar in enumerate(bars):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height/2, f"{height:.0f}ms",
                ha='center', color='white', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}typing_time_1key.png')
    plt.close()

def visualize_left_home_key_error_vs_time(key_df, min_count=10):
    """
    Create a scatter plot comparing error rates vs. typing times for individual left home block keys
    Parameters:
    key_df (DataFrame): DataFrame containing individual key statistics
    min_count (int): Minimum count threshold for visualization
    """
    # Filter for sufficient data
    filtered_df = key_df[key_df['totalCount'] > min_count].copy()
    if filtered_df.empty:
        print("No data available for key error vs. time visualization")
        return
    
    # Create output directory for figures
    output_dir = 'output/figures/'
    os.makedirs(output_dir, exist_ok=True)
    
    # Convert keys to uppercase for display
    filtered_df['key_upper'] = filtered_df['key'].str.upper()
    
    # Create scatter plot with error bars
    plt.figure(figsize=(10, 8))
    
    plt.errorbar(
        filtered_df['medianTime'], 
        filtered_df['errorRate'],
        xerr=filtered_df['timeMAD'], 
        yerr=filtered_df['errorMAD'],
        fmt='o', 
        alpha=0.7, 
        ecolor='lightgray', 
        capsize=3
    )
    
    # Add key labels, displaced from markers
    for _, row in filtered_df.iterrows():
        plt.annotate(
            row['key_upper'], 
            (row['medianTime'], row['errorRate']),
            xytext=(8, 0), 
            textcoords='offset points',
            fontsize=12, 
            ha='left', 
            va='center'
        )
    
    plt.title('Error rate vs. typing time for left home block keys')
    plt.xlabel('Median typing time (ms)')
    plt.ylabel('Error rate')
    plt.xlim(100, 400)
    plt.grid(True, alpha=0.3)
    
    # Add correlation statistics if there are enough data points
    if len(filtered_df) > 3:
        try:
            # Check if there's variation in both variables
            if (filtered_df['errorRate'].std() > 0 and 
                filtered_df['medianTime'].std() > 0):
                
                # Calculate correlation and p-value
                corr, p_value = stats.pearsonr(
                    filtered_df['medianTime'], 
                    filtered_df['errorRate']
                )
                
                # Add correlation text to plot
                plt.text(
                    0.05, 0.95, 
                    f"correlation: {corr:.2f}\np-value: {p_value:.3f}",
                    transform=plt.gca().transAxes,
                    fontsize=10,
                    va='top',
                    bbox=dict(facecolor='white', alpha=0.7)
                )
                
                # Add trend line
                x = filtered_df['medianTime']
                y = filtered_df['errorRate']
                z = np.polyfit(x, y, 1)
                p = np.poly1d(z)
                plt.plot(x, p(x), "r--", alpha=0.8)
        except Exception as e:
            print(f"Could not calculate correlation: {e}")
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}error_vs_typing_time_1key.png')
    plt.close()

#-------------------------------------------------------------------------------
# Analyze key-pairs
#-------------------------------------------------------------------------------
def visualize_left_home_bigram_typing_times(bigram_df, min_count=10):
    """
    Create a visualization of median typing times for left home block bigrams
    Parameters:
    bigram_df (DataFrame): DataFrame containing bigram statistics
    min_count (int): Minimum count threshold for visualization
    """
    # Filter for sufficient data with timing samples
    filtered_df = bigram_df[(bigram_df['totalCount'] > min_count) & 
                            (bigram_df['medianTime'] > 0)].copy()
    if filtered_df.empty:
        print("No data available for bigram typing time visualization")
        return
    
    # Create output directory for figures
    output_dir = 'output/figures/'
    os.makedirs(output_dir, exist_ok=True)
    
    # Sort by median time for visualization
    filtered_df = filtered_df.sort_values('medianTime', ascending=False)
    
    # Create median time plot with MAD error bars
    plt.figure(figsize=(20, 8))
    
    # Convert bigrams to uppercase for display
    filtered_df['bigram_upper'] = filtered_df['bigram'].str.upper()
    
    # Create the bar plot
    bars = plt.bar(filtered_df['bigram_upper'], filtered_df['medianTime'])
    
    # Add error bars using time MAD
    plt.errorbar(
        x=range(len(filtered_df)), 
        y=filtered_df['medianTime'],
        yerr=filtered_df['timeMAD'], 
        fmt='none', 
        color='black', 
        capsize=5
    )
    
    plt.title('Typing times for left home block bigrams')
    plt.xlabel('Bigram')
    plt.ylabel('Median typing time (ms)')
    plt.xticks(rotation=90)  # Vertical labels
    
    # Add time labels
    #for i, bar in enumerate(bars):
    #    height = bar.get_height()
    #    plt.text(bar.get_x() + bar.get_width()/2., height/2, f"{height:.0f}ms",
    #            ha='center', color='white', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}typing_time_2key.png')
    plt.close()

def visualize_left_home_bigram_error_rates(bigram_df, min_count=10):
    """
    Create a visualization of error rates for left home block bigrams
    Parameters:
    bigram_df (DataFrame): DataFrame containing bigram statistics
    min_count (int): Minimum count threshold for visualization
    """
    # Filter for sufficient data
    filtered_df = bigram_df[bigram_df['totalCount'] > min_count].copy()
    
    # Count total bigrams before filtering for errors
    total_bigrams = len(filtered_df)
    
    # Remove bigrams with zero error rates
    filtered_df = filtered_df[filtered_df['errorRate'] > 0]
    
    # Count shown vs not shown
    shown_bigrams = len(filtered_df)
    not_shown_bigrams = total_bigrams - shown_bigrams
    
    if filtered_df.empty:
        print("No data available for bigram error rate visualization after filtering")
        return
    
    # Create output directory for figures
    output_dir = 'output/figures/'
    os.makedirs(output_dir, exist_ok=True)
    
    # Sort by error rate for visualization
    filtered_df = filtered_df.sort_values('errorRate', ascending=False)
    
    # Create error rate plot with MAD error bars
    plt.figure(figsize=(24, 16))
    
    # Convert bigrams to uppercase for display
    filtered_df['bigram_upper'] = filtered_df['bigram'].str.upper()
    
    # Create the bar plot
    bars = plt.bar(filtered_df['bigram_upper'], filtered_df['errorRate'])
    
    # Add error bars using error MAD
    plt.errorbar(
        x=range(len(filtered_df)), 
        y=filtered_df['errorRate'],
        yerr=filtered_df['errorMAD'], 
        fmt='none', 
        color='black', 
        capsize=5
    )
    
    plt.title('Error rates for left home block bigrams')
    plt.xlabel('bigram')
    plt.ylabel('error rate')
    plt.xticks(rotation=90)  # Vertical labels
    plt.ylim(0, 0.1)

    # Add percentage labels
    for i, bar in enumerate(bars):
        height = bar.get_height()
        if height > 0.001:  # Only add labels if error rate is non-negligible
            plt.text(bar.get_x() + bar.get_width()/2., height/2, f"{height*100:.1f}%",
                    ha='center', color='white', fontweight='bold')
    
    # Add count of shown vs. not shown
    plt.text(
        0.75, 0.95,
        f"showing {shown_bigrams} bigrams\n(excluded {not_shown_bigrams} with no errors)",
        transform=plt.gca().transAxes,
        fontsize=10,
        va='top',
        bbox=dict(facecolor='white', alpha=0.7)
    )
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}error_2key.png')
    plt.close()

def visualize_left_home_bigram_error_vs_time(bigram_df, min_count=10):
    """
    Create a scatter plot comparing error rates vs. typing times for left home block bigrams
    Parameters:
    bigram_df (DataFrame): DataFrame containing bigram statistics
    min_count (int): Minimum count threshold for visualization
    """
    # Filter for sufficient data
    initial_df = bigram_df[(bigram_df['totalCount'] > min_count) & 
                          (bigram_df['medianTime'] > 0)].copy()
    
    # Count total bigrams before filtering for errors
    total_bigrams = len(initial_df)
    
    # Remove bigrams with zero error rates
    filtered_df = initial_df[initial_df['errorRate'] > 0]
    
    # Count shown vs not shown
    shown_bigrams = len(filtered_df)
    not_shown_bigrams = total_bigrams - shown_bigrams
    
    if filtered_df.empty:
        print("No data available for bigram error vs. time visualization after filtering")
        return
    
    # Create output directory for figures
    output_dir = 'output/figures/'
    os.makedirs(output_dir, exist_ok=True)
    
    # Convert bigrams to uppercase for display
    filtered_df['bigram_upper'] = filtered_df['bigram'].str.upper()
    
    # Create scatter plot with error bars
    plt.figure(figsize=(12, 10))
    
    # Calculate more aggressive axis limits to spread data
    max_time = filtered_df['medianTime'].max() * 1.05
    max_error = filtered_df['errorRate'].max() * 1.05
    
    plt.errorbar(
        filtered_df['medianTime'], 
        filtered_df['errorRate'],
        xerr=filtered_df['timeMAD'], 
        yerr=filtered_df['errorMAD'],
        fmt='o', 
        alpha=0.7, 
        ecolor='lightgray', 
        capsize=2  # Reduced for less overlap
    )
    
    # Add bigram labels, displaced from markers
    for _, row in filtered_df.iterrows():
        plt.annotate(
            row['bigram_upper'], 
            (row['medianTime'], row['errorRate']),
            xytext=(8, 0), 
            textcoords='offset points',
            fontsize=12, 
            ha='left', 
            va='center'
        )
    
    plt.title('Error rate vs. typing time for left home block bigrams')
    plt.xlabel('Median typing time (ms)')
    plt.ylabel('Error rate')
    plt.grid(True, alpha=0.3)
    
    # Set tighter axis limits to spread data more
    plt.xlim(100, 400)
    #plt.xlim(0, max_time)
    #plt.ylim(-0.01, max_error)
    plt.ylim(-0.01, 0.1)
    
    # Add count of shown vs. not shown - moved to right side
    plt.text(
        0.75, 0.85,
        f"showing {shown_bigrams} bigrams\n(excluded {not_shown_bigrams} with no errors)",
        transform=plt.gca().transAxes,
        fontsize=10,
        va='top',
        bbox=dict(facecolor='white', alpha=0.7)
    )
    
    # Add correlation statistics if there are enough data points
    if len(filtered_df) > 3:
        try:
            # Check if there's variation in both variables
            if (filtered_df['errorRate'].std() > 0 and 
                filtered_df['medianTime'].std() > 0):
                
                # Calculate correlation and p-value
                corr, p_value = stats.pearsonr(
                    filtered_df['medianTime'], 
                    filtered_df['errorRate']
                )
                
                # Add correlation text to plot - moved to right side
                plt.text(
                    0.75, 0.95, 
                    f"correlation: {corr:.2f}\np-value: {p_value:.3f}",
                    transform=plt.gca().transAxes,
                    fontsize=10,
                    va='top',
                    bbox=dict(facecolor='white', alpha=0.7)
                )
                
                # Add trend line
                x = filtered_df['medianTime']
                y = filtered_df['errorRate']
                z = np.polyfit(x, y, 1)
                p = np.poly1d(z)
                plt.plot(x, p(x), "r--", alpha=0.8)
        except Exception as e:
            print(f"Could not calculate correlation: {e}")
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}error_vs_typing_time_2key.png')
    plt.close()

def visualize_left_home_bigram_counts(bigram_df):
    """
    Create visualization of bigram counts
    Parameters:
    bigram_df (DataFrame): DataFrame containing bigram statistics
    """
    # Filter for sufficient data
    min_count = 10
    filtered_df = bigram_df[bigram_df['totalCount'] > min_count].copy()
    if filtered_df.empty:
        print("No data available for bigram visualization")
        return
    
    # Create output directory for figures
    output_dir = 'output/figures/'
    os.makedirs(output_dir, exist_ok=True)
    
    # Bigram total counts
    plt.figure(figsize=(20, 8))
    # Sort by count for better visualization
    count_df = filtered_df.sort_values('totalCount', ascending=False)
    bars = plt.bar(count_df['bigram'].str.upper(), count_df['totalCount'])
    
    plt.title('Number of flanked bigram occurrences')
    plt.xlabel('Bigram')
    plt.ylabel('Count')
    plt.xticks(rotation=90)  # Make tick labels vertical instead of slanted
        
    plt.tight_layout()
    plt.savefig(f'{output_dir}count_bigram.png')
    plt.close()

#-------------------------------------------------------------------------------
# Analyze 1-key vs. key-pairs
#-------------------------------------------------------------------------------
def visualize_key_vs_bigram_errors(key_df, bigram_df, min_count=10):
    """
    Create a scatter plot comparing 1-key error rates vs 2-key error rates
    Parameters:
    key_df (DataFrame): DataFrame containing individual key statistics
    bigram_df (DataFrame): DataFrame containing bigram statistics
    min_count (int): Minimum count threshold for visualization
    """
    # Create output directory for figures
    output_dir = 'output/figures/'
    os.makedirs(output_dir, exist_ok=True)
    
    # Make a copy of the DataFrames for processing
    key_df_copy = key_df.copy()
    bigram_df_copy = bigram_df.copy()
    
    # Extract the second key from each bigram
    bigram_df_copy['second_key'] = bigram_df_copy['bigram'].str[1]
    
    # Filter for sufficient data
    filtered_keys = key_df_copy[key_df_copy['totalCount'] > min_count]
    filtered_bigrams = bigram_df_copy[(bigram_df_copy['totalCount'] > min_count)]
    
    # Create a DataFrame with matched pairs of 1-key and 2-key stats
    paired_data = []
    
    for _, key_row in filtered_keys.iterrows():
        key = key_row['key']
        key_upper = key.upper()
        
        # Find all bigrams where this key is the second key
        matching_bigrams = filtered_bigrams[filtered_bigrams['second_key'] == key]
        
        for _, bigram_row in matching_bigrams.iterrows():
            bigram = bigram_row['bigram']
            bigram_upper = bigram.upper()
            
            # Skip pairs where both error rates are zero
            if key_row['errorRate'] == 0 and bigram_row['errorRate'] == 0:
                continue
                
            paired_data.append({
                'key': key,
                'key_upper': key_upper,
                'bigram': bigram,
                'bigram_upper': bigram_upper,
                'key_errorRate': key_row['errorRate'],
                'key_errorMAD': key_row['errorMAD'],
                'bigram_errorRate': bigram_row['errorRate'],
                'bigram_errorMAD': bigram_row['errorMAD']
            })
    
    # Count shown vs not shown
    total_possible_pairs = len(filtered_keys) * len(filtered_bigrams.drop_duplicates('second_key'))
    shown_pairs = len(paired_data)
    not_shown_pairs = total_possible_pairs - shown_pairs
    
    if not paired_data:
        print("No matching key-bigram pairs found for error comparison")
        return
    
    paired_df = pd.DataFrame(paired_data)
    
    # Create the scatter plot with wider aspect ratio
    plt.figure(figsize=(14, 8))
    
    # Add points with error bars
    plt.errorbar(
        paired_df['key_errorRate'], 
        paired_df['bigram_errorRate'],
        xerr=paired_df['key_errorMAD'], 
        yerr=paired_df['bigram_errorMAD'],
        fmt='o', 
        alpha=0.7, 
        ecolor='lightgray', 
        capsize=3
    )
    
    # Determine axis limits with more horizontal spread
    max_x = paired_df['key_errorRate'].max() * 1.3 if paired_df['key_errorRate'].max() > 0 else 0.01
    max_y = paired_df['bigram_errorRate'].max() * 1.1 if paired_df['bigram_errorRate'].max() > 0 else 0.01
    max_error = max(max_x, max_y)
    
    # Add diagonal line (y=x)
    plt.plot([0, max_error], [0, max_error], 'k--', alpha=0.5, label='equal error rate')
    
    # Add labels for each point
    for _, row in paired_df.iterrows():
        plt.annotate(
            f"{row['bigram_upper']}",
            (row['key_errorRate'], row['bigram_errorRate']),
            xytext=(5, 5), 
            textcoords='offset points',
            fontsize=10
        )
    
    plt.title('Error rates: 1-key vs. 2-key')
    plt.xlabel('1-key error rate')
    plt.ylabel('2-key error rate')
    plt.grid(True, alpha=0.3)
    
    # Add count of shown vs. not shown - moved to right side
    plt.text(
        0.75, 0.80,
        f"showing {shown_pairs} bigram pairs\n(excluded {not_shown_pairs} with no errors)",
        transform=plt.gca().transAxes,
        fontsize=10,
        va='bottom',
        bbox=dict(facecolor='white', alpha=0.7)
    )
    
    # Set axis limits to focus on data with more horizontal spread
    plt.xlim(0, max_x)
    plt.ylim(0, max_y)
    
    # Calculate correlation
    if len(paired_df) > 3:
        try:
            # Check if there's variation in both variables
            if (paired_df['key_errorRate'].std() > 0 and 
                paired_df['bigram_errorRate'].std() > 0):
                
                corr, p_value = stats.pearsonr(
                    paired_df['key_errorRate'], 
                    paired_df['bigram_errorRate']
                )
                
                plt.text(
                    0.75, 0.95, 
                    f"correlation: {corr:.2f}\np-value: {p_value:.3f}",
                    transform=plt.gca().transAxes,
                    fontsize=10,
                    va='top',
                    bbox=dict(facecolor='white', alpha=0.7)
                )
                
                # Add trend line
                x = paired_df['key_errorRate']
                y = paired_df['bigram_errorRate']
                z = np.polyfit(x, y, 1)
                p = np.poly1d(z)
                plt.plot(x, p(x), "r--", alpha=0.8)
        except Exception as e:
            print(f"Could not calculate correlation for errors: {e}")
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}error_1key_vs_2key.png')
    plt.close()

def visualize_key_vs_bigram_times(key_df, bigram_df, min_count=10):
    """
    Create a scatter plot comparing 1-key times vs 2-key times
    Parameters:
    key_df (DataFrame): DataFrame containing individual key statistics
    bigram_df (DataFrame): DataFrame containing bigram statistics
    min_count (int): Minimum count threshold for visualization
    """
    # Create output directory for figures
    output_dir = 'output/figures/'
    os.makedirs(output_dir, exist_ok=True)
    
    # Make a copy of the DataFrames for processing
    key_df_copy = key_df.copy()
    bigram_df_copy = bigram_df.copy()
    
    # Extract the second key from each bigram
    bigram_df_copy['second_key'] = bigram_df_copy['bigram'].str[1]
    
    # Filter for sufficient data
    filtered_keys = key_df_copy[key_df_copy['totalCount'] > min_count]
    filtered_bigrams = bigram_df_copy[(bigram_df_copy['totalCount'] > min_count) & 
                                     (bigram_df_copy['medianTime'] > 0)]
    
    # Create a DataFrame with matched pairs of 1-key and 2-key stats
    paired_data = []
    
    for _, key_row in filtered_keys.iterrows():
        key = key_row['key']
        key_upper = key.upper()
        
        # Find all bigrams where this key is the second key
        matching_bigrams = filtered_bigrams[filtered_bigrams['second_key'] == key]
        
        for _, bigram_row in matching_bigrams.iterrows():
            bigram = bigram_row['bigram']
            bigram_upper = bigram.upper()
            
            paired_data.append({
                'key': key,
                'key_upper': key_upper,
                'bigram': bigram,
                'bigram_upper': bigram_upper,
                'key_medianTime': key_row['medianTime'],
                'key_timeMAD': key_row['timeMAD'],
                'bigram_medianTime': bigram_row['medianTime'],
                'bigram_timeMAD': bigram_row['timeMAD']
            })
    
    if not paired_data:
        print("No matching key-bigram pairs found for timing comparison")
        return
    
    paired_df = pd.DataFrame(paired_data)
    
    # Create the scatter plot
    plt.figure(figsize=(12, 10))
    
    # Calculate more aggressive axis limits to spread data
    max_x = paired_df['key_medianTime'].max() * 1.05
    max_y = paired_df['bigram_medianTime'].max() * 1.05
    
    # Add points with error bars
    plt.errorbar(
        paired_df['key_medianTime'], 
        paired_df['bigram_medianTime'],
        xerr=paired_df['key_timeMAD'], 
        yerr=paired_df['bigram_timeMAD'],
        fmt='o', 
        alpha=0.7, 
        ecolor='lightgray', 
        capsize=2  # Reduced for less overlap
    )
    
    # Add diagonal line (y=x)
    plt.plot([0, max(max_x, max_y)], [0, max(max_x, max_y)], 'k--', alpha=0.5, label='Equal Time')
    
    # Add labels for each point
    for _, row in paired_df.iterrows():
        plt.annotate(
            f"{row['bigram_upper']}",
            (row['key_medianTime'], row['bigram_medianTime']),
            xytext=(5, 5), 
            textcoords='offset points',
            fontsize=10
        )
    
    plt.title('Typing times: 1-key vs. 2-key')
    plt.xlabel('1-key median typing time (ms)')
    plt.ylabel('2-key median typing time (ms)')
    plt.grid(True, alpha=0.3)
    
    # Set tighter axis limits to spread data more
    plt.xlim(100, 400)
    plt.ylim(50, 400)
    #plt.xlim(0, max_x)
    #plt.ylim(0, max_y)
    
    # Add region labels in better positions to avoid overlap
    plt.text(max_x*0.2, max_y*0.9, '2-key time > 1-key time',
             ha='center', fontsize=12, alpha=0.7)
    plt.text(max_x*0.9, max_y*0.2, '1-key time > 2-key time',
             ha='center', fontsize=12, alpha=0.7)
    
    # Calculate correlation
    if len(paired_df) > 3:
        try:
            # Check if there's variation in both variables
            if (paired_df['key_medianTime'].std() > 0 and 
                paired_df['bigram_medianTime'].std() > 0):
                
                corr, p_value = stats.pearsonr(
                    paired_df['key_medianTime'], 
                    paired_df['bigram_medianTime']
                )
                
                plt.text(
                    0.75, 0.95, 
                    f"correlation: {corr:.2f}\np-value: {p_value:.3f}",
                    transform=plt.gca().transAxes,
                    fontsize=12,
                    va='top',
                    bbox=dict(facecolor='white', alpha=0.7)
                )
                
                # Add trend line
                x = paired_df['key_medianTime']
                y = paired_df['bigram_medianTime']
                z = np.polyfit(x, y, 1)
                p = np.poly1d(z)
                plt.plot(x, p(x), "r--", alpha=0.8)
        except Exception as e:
            print(f"Could not calculate correlation for times: {e}")
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}typing_time_1key_vs_2key.png')
    plt.close()
    
#-------------------------------------------------------------------------------
# Mirrored key pairs
#-------------------------------------------------------------------------------
def analyze_mirror_pairs(key_stats, letter_freq=None):
    """
    Analyze mirror image key pairs using the new key stats definitions
    Parameters:
    key_stats (dict): Dictionary of key statistics from analyze_key_stats
    letter_freq (DataFrame): Optional letter frequency data for regression
    Returns:
    DataFrame: Comparison of mirror pair statistics
    """
    mirror_data = []
    # Create frequency dictionary if provided
    freq_dict = {}
    if letter_freq is not None:
        # Try different column names for letter frequency data
        if 'item' in letter_freq.columns and 'score' in letter_freq.columns:
            for _, row in letter_freq.iterrows():
                letter_value = str(row['item']).lower()
                if len(letter_value) == 1:  # Only use single characters
                    freq_dict[letter_value] = row['score']
        else:
            key_col = None
            freq_col = None
            # Try to find letter column
            for col in ['letter', 'char', 'character', 'item', 'key']:
                if col in letter_freq.columns:
                    key_col = col
                    break
            # Try to find frequency column
            for col in ['frequency', 'freq', 'count', 'score', 'probability']:
                if col in letter_freq.columns:
                    freq_col = col
                    break
            if key_col and freq_col:
                for _, row in letter_freq.iterrows():
                    letter_value = str(row[key_col]).lower()
                    if len(letter_value) == 1:  # Only use single characters
                        freq_dict[letter_value] = row[freq_col]
    
    # Filter mirror pairs to only include home block keys
    home_block_mirror_pairs = []
    for left, right in MIRROR_PAIRS:
        if left in LEFT_HOME_KEYS:
            home_block_mirror_pairs.append((left, right))
    
    for left, right in home_block_mirror_pairs:
        if left in key_stats and right in key_stats:
            left_stats = key_stats[left]
            right_stats = key_stats[right]
            # Only include pairs where both keys have sufficient data
            if left_stats['totalCount'] > 10 and right_stats['totalCount'] > 10:
                pair_data = {
                    'left_key': left,
                    'right_key': right,
                    'left_count': left_stats['totalCount'],
                    'right_count': right_stats['totalCount'],
                    'left_error_rate': left_stats['errorRate'],
                    'right_error_rate': right_stats['errorRate'],
                    'left_median_time': left_stats['medianTime'],
                    'right_median_time': right_stats['medianTime'],
                    'left_time_mad': left_stats['timeMAD'],
                    'right_time_mad': right_stats['timeMAD'],
                    'left_error_mad': left_stats['errorMAD'],
                    'right_error_mad': right_stats['errorMAD'],
                    'left_most_common_mistype': left_stats['most_common_mistype'],
                    'right_most_common_mistype': right_stats['most_common_mistype'],
                    'error_diff': left_stats['errorRate'] - right_stats['errorRate'],
                    'time_diff': left_stats['medianTime'] - right_stats['medianTime']
                }
                # Add frequency info if available
                if freq_dict:
                    pair_data['left_freq'] = freq_dict.get(left, 0)
                    pair_data['right_freq'] = freq_dict.get(right, 0)
                mirror_data.append(pair_data)
    mirror_df = pd.DataFrame(mirror_data)
    # If frequency data is available, adjust mirror pair metrics
    if 'left_freq' in mirror_df.columns and 'right_freq' in mirror_df.columns:
        mirror_df = adjust_mirror_pairs_for_frequency(mirror_df)
    return mirror_df

def visualize_mirror_pairs(mirror_df):
    """
    Create visualizations comparing mirror image key pairs
    Parameters:
    mirror_df (DataFrame): DataFrame of mirror pair comparison statistics
    """
    if mirror_df.empty:
        print("No data available for mirror pairs visualization")
        return
    # Create output directory for figures
    output_dir = 'output/figures/'
    os.makedirs(output_dir, exist_ok=True)
    # Sort mirror pairs for consistent visualization
    mirror_df = mirror_df.sort_values('left_key')
    
    # Convert keys to uppercase for display
    mirror_df['left_key_upper'] = mirror_df['left_key'].str.upper()
    mirror_df['right_key_upper'] = mirror_df['right_key'].str.upper()
    
    # 1. Error rate comparison with MAD error bars
    plt.figure(figsize=(15, 8))
    # Prepare data for grouped bar plot
    error_data = []
    for _, row in mirror_df.iterrows():
        error_data.append({'pair': f"{row['left_key_upper']}-{row['right_key_upper']}",
                           'position': 'Left',
                           'error_rate': row['left_error_rate'],
                           'error_mad': row['left_error_mad']})
        error_data.append({'pair': f"{row['left_key_upper']}-{row['right_key_upper']}",
                           'position': 'Right',
                           'error_rate': row['right_error_rate'],
                           'error_mad': row['right_error_mad']})
    error_df = pd.DataFrame(error_data)
    # Create grouped bar plot with error bars
    ax = sns.barplot(x='pair', y='error_rate', hue='position', data=error_df)
    # Add error bars manually - with index safety check
    for i, p in enumerate(ax.patches):
        if i < len(error_df):  # Add safety check
            row = error_df.iloc[i]
            height = p.get_height()
            ax.errorbar(p.get_x() + p.get_width()/2., height,
                        yerr=row['error_mad'], fmt='none', color='black', capsize=3)
            ax.text(p.get_x() + p.get_width()/2., height/2, f"{height*100:.1f}%",
                    ha='center', color='white', fontweight='bold')
    plt.title('error rate for mirror letter pairs')
    plt.xlabel('mirror letter pair')
    plt.ylabel('error rate')
    plt.xticks(rotation=45)
    plt.legend(title='key position')
    plt.tight_layout()
    plt.savefig(f'{output_dir}mirror_pair_error_rates_with_mad.png')
    plt.close()
    
    # 2. Typing time comparison with MAD error bars
    plt.figure(figsize=(15, 8))
    # Prepare data for grouped bar plot
    time_data = []
    for _, row in mirror_df.iterrows():
        time_data.append({'pair': f"{row['left_key_upper']}-{row['right_key_upper']}",
                          'position': 'Left',
                          'median_time': row['left_median_time'],
                          'time_mad': row['left_time_mad']})
        time_data.append({'pair': f"{row['left_key_upper']}-{row['right_key_upper']}",
                          'position': 'Right',
                          'median_time': row['right_median_time'],
                          'time_mad': row['right_time_mad']})
    time_df = pd.DataFrame(time_data)
    # Create grouped bar plot with error bars
    ax = sns.barplot(x='pair', y='median_time', hue='position', data=time_df)
    # Add error bars manually - with index safety check
    for i, p in enumerate(ax.patches):
        if i < len(time_df):  # Add safety check
            row = time_df.iloc[i]
            height = p.get_height()
            ax.errorbar(p.get_x() + p.get_width()/2., height,
                        yerr=row['time_mad'], fmt='none', color='black', capsize=3)
            ax.text(p.get_x() + p.get_width()/2., height/2, f"{height:.0f}ms",
                    ha='center', color='white', fontweight='bold')
    plt.title('typing times for left-right mirror letter pairs')
    plt.xlabel('mirror letter pair')
    plt.ylabel('median typing time (ms)')
    plt.xticks(rotation=45)
    plt.legend(title='key position')
    plt.tight_layout()
    plt.savefig(f'{output_dir}mirror_pair_typing_time.png')
    plt.close()
    
    # 3. Scatter plot comparing left vs right error rates with MAD error bars
    plt.figure(figsize=(10, 10))
    # Check if there's variance in the data
    error_range = max(
        mirror_df['left_error_rate'].max() - mirror_df['left_error_rate'].min(),
        mirror_df['right_error_rate'].max() - mirror_df['right_error_rate'].min()
    )
    
    # Only create plot if there's enough variance in the data
    if error_range > 0.001:  # Using a small threshold to determine if data varies
        # Create scatter plot with error bars
        plt.errorbar(
            mirror_df['right_error_rate'], mirror_df['left_error_rate'],
            xerr=mirror_df['right_error_mad'], yerr=mirror_df['left_error_mad'],
            fmt='o', alpha=0.7, ecolor='lightgray', capsize=3
        )
        # Add diagonal line representing equal error rates
        max_error = max(mirror_df['left_error_rate'].max(), mirror_df['right_error_rate'].max()) * 1.1
        min_error = min(mirror_df['left_error_rate'].min(), mirror_df['right_error_rate'].min()) * 0.9
        if max_error > min_error:
            plt.plot([min_error, max_error], [min_error, max_error], 'k--', alpha=0.5, label='equal error rate')
        
        # Add point labels for each mirror pair
        for _, row in mirror_df.iterrows():
            plt.annotate(
                f"{row['left_key_upper']}-{row['right_key_upper']}",
                (row['right_error_rate'], row['left_error_rate']),
                xytext=(5, 5), textcoords='offset points',
                fontsize=10
            )
        
        # Add region labels if there's sufficient range
        if max_error > min_error * 1.1:
            plt.text(max_error*0.25, max_error*0.75, 'left keys more error-prone',
                     ha='center', fontsize=12, alpha=0.7)
            plt.text(max_error*0.75, max_error*0.25, 'right keys more error-prone',
                     ha='center', fontsize=12, alpha=0.7)
        
        plt.title('error rates for left-right mirror letter pairs')
        plt.ylabel('left key error rate')
        plt.xlabel('right key error rate')
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.savefig(f'{output_dir}mirror_pair_error_left_vs_right.png')
    else:
        print("Skipping error rate scatter plot - insufficient variance in data")
    
    plt.close()
    
    # 4. Scatter plot comparing left vs right typing times with MAD and frequency adjustment
    plt.figure(figsize=(10, 10))
    # Check if there's variance in the data
    time_range = max(
        mirror_df['left_median_time'].max() - mirror_df['left_median_time'].min(),
        mirror_df['right_median_time'].max() - mirror_df['right_median_time'].min()
    )
    
    if time_range > 1.0:  # Only create plot if there's at least 1ms of variance
        # Create scatter plot with error bars
        plt.errorbar(
            mirror_df['right_median_time'], mirror_df['left_median_time'],
            xerr=mirror_df['right_time_mad'], yerr=mirror_df['left_time_mad'],
            fmt='o', alpha=0.7, ecolor='lightgray', capsize=3,
            label='raw'
        )
        # Add diagonal line representing equal typing times
        max_time = max(mirror_df['left_median_time'].max(), mirror_df['right_median_time'].max()) * 1.1
        min_time = min(mirror_df['left_median_time'].min(), mirror_df['right_median_time'].min()) * 0.9
        if max_time > min_time:
            plt.plot([min_time, max_time], [min_time, max_time], 'k--', alpha=0.5, label='equal typing time')
        
        # Add point labels for each mirror pair
        for _, row in mirror_df.iterrows():
            plt.annotate(
                f"{row['left_key_upper']}-{row['right_key_upper']}",
                (row['right_median_time'], row['left_median_time']),
                xytext=(5, 5), textcoords='offset points',
                fontsize=10
            )
        
        # Add frequency-adjusted points if available
        if 'left_median_time_freq_adjusted' in mirror_df.columns:
            plt.scatter(
                mirror_df['right_median_time_freq_adjusted'], mirror_df['left_median_time_freq_adjusted'],
                marker='x', color='red', alpha=0.7, s=50,
                label='frequency-adjusted'
            )
            # Connect raw and adjusted points with lines
            for _, row in mirror_df.iterrows():
                plt.plot(
                    [row['right_median_time'], row['right_median_time_freq_adjusted']],
                    [row['left_median_time'], row['left_median_time_freq_adjusted']],
                    'r-', alpha=0.3
                )
        
        # Add region labels if there's sufficient range
        if max_time > min_time * 1.1:
            plt.text(max_time*0.25, max_time*0.75, 'left keys slower',
                     ha='center', fontsize=12, alpha=0.7)
            plt.text(max_time*0.75, max_time*0.25, 'right keys slower',
                     ha='center', fontsize=12, alpha=0.7)
        
        plt.title('typing times for left-right mirror letter pairs')
        plt.ylabel('left key median typing time (ms)')
        plt.xlabel('right key median typing time (ms)')
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.tight_layout()
        plt.savefig(f'{output_dir}mirror_pair_time_left_vs_right.png')
    else:
        print("Skipping time scatter plot - insufficient variance in data")
    
    plt.close()

def adjust_mirror_pairs_for_frequency(mirror_df):
    """
    Adjust mirror pair metrics by regressing out frequency effects
    Parameters:
    mirror_df (DataFrame): DataFrame of mirror pair comparison statistics
    Returns:
    DataFrame: DataFrame with frequency-adjusted metrics
    """
    # Check if frequency data is available
    if 'left_freq' not in mirror_df.columns or 'right_freq' not in mirror_df.columns:
        print("No frequency data available for adjustment")
        return mirror_df
    # Create a copy of the input DataFrame
    adjusted_df = mirror_df.copy()
    # Add log-transformed frequency (common in psycholinguistic research)
    adjusted_df['left_log_freq'] = np.log10(adjusted_df['left_freq'] + 1)  # Add 1 to handle zeros
    adjusted_df['right_log_freq'] = np.log10(adjusted_df['right_freq'] + 1)
    # Metrics to adjust
    left_metrics = ['left_error_rate', 'left_median_time']
    right_metrics = ['right_error_rate', 'right_median_time']
    # Adjust left key metrics
    for metric in left_metrics:
        if metric in adjusted_df.columns:
            try:
                # Create the model: metric ~ log_frequency
                X = sm.add_constant(adjusted_df['left_log_freq'])
                y = adjusted_df[metric]
                # Fit the model
                model = sm.OLS(y, X).fit()
                # Get the model parameters
                intercept = model.params[0]
                slope = model.params[1]
                # Calculate predicted values
                adjusted_df[f'{metric}_pred'] = intercept + slope * adjusted_df['left_log_freq']
                # Calculate residuals (the frequency-adjusted values) + mean
                adjusted_df[f'{metric}_freq_adjusted'] = adjusted_df[metric] - adjusted_df[f'{metric}_pred'] + adjusted_df[metric].mean()
                print(f"Adjusted {metric} for left keys (R² = {model.rsquared:.3f})")
            except Exception as e:
                print(f"Error adjusting {metric} for left keys: {e}")
    # Adjust right key metrics
    for metric in right_metrics:
        if metric in adjusted_df.columns:
            try:
                # Create the model: metric ~ log_frequency
                X = sm.add_constant(adjusted_df['right_log_freq'])
                y = adjusted_df[metric]
                # Fit the model
                model = sm.OLS(y, X).fit()
                # Get the model parameters
                intercept = model.params[0]
                slope = model.params[1]
                # Calculate predicted values
                adjusted_df[f'{metric}_pred'] = intercept + slope * adjusted_df['right_log_freq']
                # Calculate residuals (the frequency-adjusted values) + mean
                adjusted_df[f'{metric}_freq_adjusted'] = adjusted_df[metric] - adjusted_df[f'{metric}_pred'] + adjusted_df[metric].mean()
                print(f"Adjusted {metric} for right keys (R² = {model.rsquared:.3f})")
            except Exception as e:
                print(f"Error adjusting {metric} for right keys: {e}")
    # Calculate adjusted differences
    if 'left_error_rate_freq_adjusted' in adjusted_df.columns and 'right_error_rate_freq_adjusted' in adjusted_df.columns:
        adjusted_df['error_diff_freq_adjusted'] = adjusted_df['left_error_rate_freq_adjusted'] - adjusted_df['right_error_rate_freq_adjusted']
    if 'left_median_time_freq_adjusted' in adjusted_df.columns and 'right_median_time_freq_adjusted' in adjusted_df.columns:
        adjusted_df['time_diff_freq_adjusted'] = adjusted_df['left_median_time_freq_adjusted'] - adjusted_df['right_median_time_freq_adjusted']
    return adjusted_df

#-------------------------------------------------------------------------------
# Generate csv files
#-------------------------------------------------------------------------------
def generate_accuracy_score_output_csv(key_df, mirror_df, output_path):
    """
    Generate a CSV file with accuracy statistics for all keys
    
    Parameters:
    key_df (DataFrame): DataFrame with key statistics for left home block
    mirror_df (DataFrame): DataFrame with mirror pair statistics
    output_path (str): Path to save the output CSV
    """
    # Create a new DataFrame for the output
    output_columns = [
        'key', 
        'frequency-adjusted accuracy', 
        'frequency-adjusted accuracy MAD', 
        'accuracy',
        'error rate', 
        'error MAD', 
        'error count', 
        'total count', 
        'most common mistype', 
        'most common mistype count'
    ]
    
    output_data = []
    
    # Add data for left home block keys
    for _, row in key_df.iterrows():
        key = row['key'].upper()
        
        # Calculate accuracy (1 - error rate)
        accuracy = 1 - row['errorRate']
        freq_adjusted_accuracy = 1 - row.get('errorRate_freq_adjusted', row['errorRate'])
        
        output_data.append({
            'key': key,
            'frequency-adjusted accuracy': freq_adjusted_accuracy,
            'frequency-adjusted accuracy MAD': row.get('errorMAD_freq_adjusted', row['errorMAD']),
            'accuracy': accuracy,
            'error rate': row['errorRate'],
            'error MAD': row['errorMAD'],
            'error count': row['errorCount'],
            'total count': row['totalCount'],
            'most common mistype': row['most_common_mistype'].upper() if row['most_common_mistype'] else '',
            'most common mistype count': row['most_common_mistype_count']
        })
    
    # Add data for right home block keys from mirror pairs
    for _, row in mirror_df.iterrows():
        left_key = row['left_key'].upper()
        right_key = row['right_key'].upper()
        
        # Find if this left key is already in our output data
        if any(d['key'] == left_key for d in output_data):
            # Calculate accuracy for right key
            right_accuracy = 1 - row['right_error_rate']
            right_freq_adjusted_accuracy = 1 - row.get('right_error_rate_freq_adjusted', row['right_error_rate'])
            
            # Add the right key data
            output_data.append({
                'key': right_key,
                'frequency-adjusted accuracy': right_freq_adjusted_accuracy,
                'frequency-adjusted accuracy MAD': row.get('right_error_mad_freq_adjusted', row['right_error_mad']),
                'accuracy': right_accuracy,
                'error rate': row['right_error_rate'],
                'error MAD': row['right_error_mad'],
                'error count': int(row['right_count'] * row['right_error_rate']),  # Estimate error count
                'total count': row['right_count'],
                'most common mistype': row['right_most_common_mistype'].upper() if row['right_most_common_mistype'] else '',
                'most common mistype count': 0  # We don't have this data for mirror pairs
            })
    
    # Create the DataFrame and save to CSV
    output_df = pd.DataFrame(output_data)
    output_df = output_df[output_columns]  # Ensure column order
    output_df.to_csv(output_path, index=False)
    print(f"Accuracy statistics saved to {output_path}")
    
    return output_df

def generate_speed_score_output_csv(key_df, mirror_df, output_path):
    """
    Generate a CSV file with typing speed statistics for all keys

    Speed MAD: Since speed is the inverse of typing time (speed = 1/time), 
    we can approximate the speed MAD using error propagation. If we have:
      speed = 1/time
      speed_median = 1/time_median
    Then a reasonable approximation for speed MAD would be:
        speed_MAD ≈ (timeMAD / time_median²)
    This follows from error propagation principles for reciprocal transformations.
    
    Parameters:
    key_df (DataFrame): DataFrame with key statistics for left home block
    mirror_df (DataFrame): DataFrame with mirror pair statistics
    output_path (str): Path to save the output CSV
    """
    # Create a new DataFrame for the output
    output_columns = [
        'key', 
        'frequency-adjusted speed', 
        'frequency-adjusted speed MAD', 
        'speed',
        'speed MAD',
        'typing time', 
        'typing time MAD', 
        'total count'
    ]
    
    output_data = []
    
    # Add data for left home block keys
    for _, row in key_df.iterrows():
        key = row['key'].upper()
        
        # Calculate speed as 1/typing time (only if typing time > 0)
        typing_time = row['medianTime']
        speed = 1000/typing_time if typing_time > 0 else 0  # Speed in keystrokes per second
        
        # Calculate speed MAD using error propagation
        speed_MAD = (row['timeMAD'] / (typing_time**2)) * 1000 if typing_time > 0 else 0
        
        # Calculate frequency-adjusted speed and its MAD
        freq_adj_time = row.get('medianTime_freq_adjusted', typing_time)
        freq_adj_speed = 1000/freq_adj_time if freq_adj_time > 0 else 0
        
        freq_adj_time_MAD = row.get('timeMAD_freq_adjusted', row['timeMAD'])
        freq_adj_speed_MAD = (freq_adj_time_MAD / (freq_adj_time**2)) * 1000 if freq_adj_time > 0 else 0
        
        output_data.append({
            'key': key,
            'frequency-adjusted speed': freq_adj_speed,
            'frequency-adjusted speed MAD': freq_adj_speed_MAD,
            'speed': speed,
            'speed MAD': speed_MAD,
            'typing time': typing_time,
            'typing time MAD': row['timeMAD'],
            'total count': row['totalCount']
        })
    
    # Add data for right home block keys from mirror pairs
    for _, row in mirror_df.iterrows():
        left_key = row['left_key'].upper()
        right_key = row['right_key'].upper()
        
        # Find if this left key is already in our output data
        if any(d['key'] == left_key for d in output_data):
            # Calculate speed metrics for right key
            right_typing_time = row['right_median_time']
            right_speed = 1000/right_typing_time if right_typing_time > 0 else 0
            
            # Calculate speed MAD using error propagation
            right_speed_MAD = (row['right_time_mad'] / (right_typing_time**2)) * 1000 if right_typing_time > 0 else 0
            
            # Calculate frequency-adjusted metrics
            right_freq_adj_time = row.get('right_median_time_freq_adjusted', right_typing_time)
            right_freq_adj_speed = 1000/right_freq_adj_time if right_freq_adj_time > 0 else 0
            
            right_freq_adj_time_MAD = row.get('right_time_mad_freq_adjusted', row['right_time_mad'])
            right_freq_adj_speed_MAD = (right_freq_adj_time_MAD / (right_freq_adj_time**2)) * 1000 if right_freq_adj_time > 0 else 0
            
            # Add the right key data
            output_data.append({
                'key': right_key,
                'frequency-adjusted speed': right_freq_adj_speed,
                'frequency-adjusted speed MAD': right_freq_adj_speed_MAD,
                'speed (keystrokes per second)': right_speed,
                'speed MAD (time MAD/median^2)': right_speed_MAD,
                'typing time (median)': right_typing_time,
                'typing time MAD': row['right_time_mad'],
                'total count': row['right_count']
            })
    
    # Create the DataFrame and save to CSV
    output_df = pd.DataFrame(output_data)
    output_df = output_df[output_columns]  # Ensure column order
    output_df.to_csv(output_path, index=False)
    print(f"Speed statistics saved to {output_path}")
    
    return output_df

def generate_bigram_accuracy_output_csv(bigram_df, output_path):
    """
    Generate a CSV file with accuracy statistics for all bigrams
    
    Parameters:
    bigram_df (DataFrame): DataFrame with bigram statistics
    output_path (str): Path to save the output CSV
    """
    # Create a new DataFrame for the output
    output_columns = [
        'bigram', 
        'frequency-adjusted accuracy', 
        'frequency-adjusted accuracy MAD', 
        'accuracy',
        'error rate', 
        'error MAD', 
        'error count', 
        'total count', 
        'most common mistype', 
        'most common mistype count'
    ]
    
    output_data = []
    
    # Add data for bigrams
    for _, row in bigram_df.iterrows():
        bigram = row['bigram'].upper()
        
        # Calculate accuracy (1 - error rate)
        accuracy = 1 - row['errorRate']
        freq_adjusted_accuracy = 1 - row.get('errorRate_freq_adjusted', row['errorRate'])
        
        output_data.append({
            'bigram': bigram,
            'frequency-adjusted accuracy': freq_adjusted_accuracy,
            'frequency-adjusted accuracy MAD': row.get('errorMAD_freq_adjusted', row['errorMAD']),
            'accuracy': accuracy,
            'error rate': row['errorRate'],
            'error MAD': row['errorMAD'],
            'error count': row['errorCount'],
            'total count': row['totalCount'],
            'most common mistype': row['most_common_mistype'].upper() if 'most_common_mistype' in row and row['most_common_mistype'] else '',
            'most common mistype count': row['most_common_mistype_count'] if 'most_common_mistype_count' in row else 0
        })
    
    # Create DataFrame and save to CSV
    output_df = pd.DataFrame(output_data, columns=output_columns)
    output_df.to_csv(output_path, index=False)
    print(f"Bigram accuracy statistics saved to {output_path}")

def generate_bigram_speed_output_csv(bigram_df, output_path):
    """
    Generate a CSV file with typing speed statistics for all bigrams
    
    Parameters:
    bigram_df (DataFrame): DataFrame with bigram statistics
    output_path (str): Path to save the output CSV
    """
    # Create a new DataFrame for the output
    output_columns = [
        'bigram', 
        'frequency-adjusted speed', 
        'frequency-adjusted speed MAD', 
        'speed',
        'speed MAD',
        'typing time', 
        'typing time MAD', 
        'total count'
    ]
    
    output_data = []
    
    # Add data for bigrams
    for _, row in bigram_df.iterrows():
        bigram = row['bigram'].upper()
        
        # Calculate speed as 1/typing time (only if typing time > 0)
        typing_time = row['medianTime']
        speed = 1000/typing_time if typing_time > 0 else 0  # Speed in keystrokes per second
        
        # Calculate speed MAD using error propagation
        speed_MAD = (row['timeMAD'] / (typing_time**2)) * 1000 if typing_time > 0 else 0
        
        # Calculate frequency-adjusted speed and its MAD
        freq_adj_time = row.get('medianTime_freq_adjusted', typing_time)
        freq_adj_speed = 1000/freq_adj_time if freq_adj_time > 0 else 0
        
        freq_adj_time_MAD = row.get('timeMAD_freq_adjusted', row['timeMAD'])
        freq_adj_speed_MAD = (freq_adj_time_MAD / (freq_adj_time**2)) * 1000 if freq_adj_time > 0 else 0
        
        output_data.append({
            'bigram': bigram,
            'frequency-adjusted speed': freq_adj_speed,
            'frequency-adjusted speed MAD': freq_adj_speed_MAD,
            'speed': speed,
            'speed MAD': speed_MAD,
            'typing time': typing_time,
            'typing time MAD': row['timeMAD'],
            'total count': row['totalCount']
        })
    
    # Create DataFrame and save to CSV
    output_df = pd.DataFrame(output_data, columns=output_columns)
    output_df.to_csv(output_path, index=False)
    print(f"Bigram speed statistics saved to {output_path}")

def generate_composite_bigram_speed_csv(bigram_df, key_df, output_path):
    """
    Generate a CSV file with bigram speed statistics where typing time is calculated
    as the sum of: (1) time to type first key + (2) time to transition from first to second key
    
    MAD calculations: error propagation for the sum of two independent measurements
    The combined MAD for the composite time is calculated using the quadrature sum: √(MAD_1² + MAD_2²).

    Composite speed MAD: error propagation for reciprocal transformations: speed_MAD ≈ (timeMAD / time²) * 1000

    Parameters:
    bigram_df (DataFrame): DataFrame with bigram statistics
    key_df (DataFrame): DataFrame with individual key statistics
    output_path (str): Path to save the output CSV
    """
    # Create a new DataFrame for the output
    output_columns = [
        'bigram', 
        'frequency-adjusted composite speed', 
        'frequency-adjusted composite speed MAD', 
        'composite speed',
        'composite speed MAD',
        'direct speed',
        'direct speed MAD',
        'composite typing time', 
        'direct typing time',
        'first key time', 
        'transition time',
        'total count'
    ]
    
    # Create a dictionary for key lookup
    key_dict = {row['key'].upper(): row for _, row in key_df.iterrows()}
    
    output_data = []
    
    # Process each bigram
    for _, row in bigram_df.iterrows():
        bigram = row['bigram'].upper()
        first_key = bigram[0]
        
        # Get direct bigram measurements
        direct_typing_time = row['medianTime']
        direct_speed = 1000/direct_typing_time if direct_typing_time > 0 else 0
        direct_speed_MAD = (row['timeMAD'] / (direct_typing_time**2)) * 1000 if direct_typing_time > 0 else 0
        
        # Get time for first key (if available)
        if first_key in key_dict:
            first_key_time = key_dict[first_key]['medianTime']
            first_key_time_MAD = key_dict[first_key]['timeMAD']
        else:
            # Skip if we don't have data for the first key
            continue
        
        # Get transition time (time from key1 to key2)
        transition_time = direct_typing_time  # This is already the transition time in the bigram_df
        transition_time_MAD = row['timeMAD']
        
        # Calculate the composite time
        composite_time = first_key_time + transition_time
        
        # Calculate composite speed (keys per second)
        composite_speed = 1000/composite_time if composite_time > 0 else 0
        
        # For the MAD, we need to propagate error from both measurements
        # Using quadrature sum for independent errors: σ_total² = σ_1² + σ_2²
        composite_time_MAD = np.sqrt(first_key_time_MAD**2 + transition_time_MAD**2)
        
        # Calculate MAD for speed using error propagation for reciprocal: σ_speed ≈ (σ_time / time²) * 1000
        composite_speed_MAD = (composite_time_MAD / (composite_time**2)) * 1000 if composite_time > 0 else 0
        
        # Frequency-adjusted calculations
        # Get frequency-adjusted times if available
        freq_adj_first_key_time = key_dict[first_key].get('medianTime_freq_adjusted', first_key_time)
        freq_adj_first_key_MAD = key_dict[first_key].get('timeMAD_freq_adjusted', first_key_time_MAD)
        
        freq_adj_transition_time = row.get('medianTime_freq_adjusted', transition_time)
        freq_adj_transition_MAD = row.get('timeMAD_freq_adjusted', transition_time_MAD)
        
        # Calculate the frequency-adjusted composite time
        freq_adj_composite_time = freq_adj_first_key_time + freq_adj_transition_time
        
        # Calculate frequency-adjusted composite speed
        freq_adj_composite_speed = 1000/freq_adj_composite_time if freq_adj_composite_time > 0 else 0
        
        # Calculate frequency-adjusted composite time MAD
        freq_adj_composite_time_MAD = np.sqrt(freq_adj_first_key_MAD**2 + freq_adj_transition_MAD**2)
        
        # Calculate frequency-adjusted composite speed MAD
        freq_adj_composite_speed_MAD = (freq_adj_composite_time_MAD / (freq_adj_composite_time**2)) * 1000 if freq_adj_composite_time > 0 else 0
        
        output_data.append({
            'bigram': bigram,
            'frequency-adjusted composite speed': freq_adj_composite_speed,
            'frequency-adjusted composite speed MAD': freq_adj_composite_speed_MAD,
            'composite speed': composite_speed,
            'composite speed MAD': composite_speed_MAD,
            'direct speed': direct_speed,
            'direct speed MAD': direct_speed_MAD,
            'composite typing time': composite_time,
            'direct typing time': direct_typing_time,
            'first key time': first_key_time,
            'transition time': transition_time,
            'total count': row['totalCount']
        })
    
    # Create DataFrame and save to CSV
    output_df = pd.DataFrame(output_data, columns=output_columns)
    output_df.to_csv(output_path, index=False)
    print(f"Composite bigram speed statistics saved to {output_path}")

def generate_instantaneous_error_rate_1key_csv(key_df, min_count=10, output_path='output/instantaneous_error_rate_1key.csv'):
    """
    Generate a CSV file with instantaneous error rate data for 1-key typing
    
    Instantaneous error rate = error rate / typing time (adjusted for frequency when available)
    
    Parameters:
    key_df (DataFrame): DataFrame containing individual key statistics
    min_count (int): Minimum count threshold for inclusion
    output_path (str): Path to save the output CSV
    """
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Filter for sufficient data
    filtered_df = key_df[key_df['totalCount'] > min_count].copy()
    if filtered_df.empty:
        print("No data available for instantaneous error rate calculation")
        return
    
    # Calculate instantaneous error rate
    # Use frequency-adjusted values when available
    filtered_df['error_rate'] = filtered_df.get('errorRate_freq_adjusted', filtered_df['errorRate'])
    filtered_df['typing_time'] = filtered_df.get('medianTime_freq_adjusted', filtered_df['medianTime'])
    
    # Avoid division by zero
    filtered_df['instantaneous_error_rate'] = filtered_df.apply(
        lambda row: row['error_rate'] / row['typing_time'] if row['typing_time'] > 0 else 0, 
        axis=1
    )
    
    # Calculate MAD for instantaneous error rate using error propagation
    # For function f(x,y) = x/y, the propagated error is approximately:
    # σ_f ≈ f * sqrt((σ_x/x)^2 + (σ_y/y)^2)
    filtered_df['error_mad'] = filtered_df.get('errorMAD_freq_adjusted', filtered_df['errorMAD'])
    filtered_df['time_mad'] = filtered_df.get('timeMAD_freq_adjusted', filtered_df['timeMAD'])
    
    filtered_df['instantaneous_error_rate_mad'] = filtered_df.apply(
        lambda row: row['instantaneous_error_rate'] * np.sqrt(
            (row['error_mad']/row['error_rate'])**2 + 
            (row['time_mad']/row['typing_time'])**2
        ) if row['error_rate'] > 0 and row['typing_time'] > 0 else 0,
        axis=1
    )
    
    # Prepare output DataFrame
    output_df = filtered_df[['key', 'error_rate', 'error_mad', 'typing_time', 'time_mad', 
                           'instantaneous_error_rate', 'instantaneous_error_rate_mad', 
                           'totalCount']].copy()
    
    # Convert keys to uppercase for display
    output_df['key'] = output_df['key'].str.upper()
    
    # Save to CSV
    output_df.to_csv(output_path, index=False)
    print(f"Instantaneous error rate data saved to {output_path}")
    
    return output_df

def generate_instantaneous_error_rate_2key_csv(bigram_df, min_count=10, output_path='output/instantaneous_error_rate_2key.csv'):
    """
    Generate a CSV file with instantaneous error rate data for 2-key (bigram) typing
    
    Instantaneous error rate = error rate / typing time (adjusted for frequency when available)
    
    Parameters:
    bigram_df (DataFrame): DataFrame containing bigram statistics
    min_count (int): Minimum count threshold for inclusion
    output_path (str): Path to save the output CSV
    """
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Filter for sufficient data with timing samples
    filtered_df = bigram_df[(bigram_df['totalCount'] > min_count) & 
                          (bigram_df['medianTime'] > 0)].copy()
    
    # Further filter for rows with non-zero error rates
    filtered_df = filtered_df[filtered_df['errorRate'] > 0]
    
    if filtered_df.empty:
        print("No data available for bigram instantaneous error rate calculation")
        return
    
    # Calculate instantaneous error rate
    # Use frequency-adjusted values when available
    filtered_df['error_rate'] = filtered_df.get('errorRate_freq_adjusted', filtered_df['errorRate'])
    filtered_df['typing_time'] = filtered_df.get('medianTime_freq_adjusted', filtered_df['medianTime'])
    
    # Avoid division by zero
    filtered_df['instantaneous_error_rate'] = filtered_df.apply(
        lambda row: row['error_rate'] / row['typing_time'] if row['typing_time'] > 0 else 0, 
        axis=1
    )
    
    # Calculate MAD for instantaneous error rate using error propagation
    # For function f(x,y) = x/y, the propagated error is approximately:
    # σ_f ≈ f * sqrt((σ_x/x)^2 + (σ_y/y)^2)
    filtered_df['error_mad'] = filtered_df.get('errorMAD_freq_adjusted', filtered_df['errorMAD'])
    filtered_df['time_mad'] = filtered_df.get('timeMAD_freq_adjusted', filtered_df['timeMAD'])
    
    filtered_df['instantaneous_error_rate_mad'] = filtered_df.apply(
        lambda row: row['instantaneous_error_rate'] * np.sqrt(
            (row['error_mad']/row['error_rate'])**2 + 
            (row['time_mad']/row['typing_time'])**2
        ) if row['error_rate'] > 0 and row['typing_time'] > 0 else 0,
        axis=1
    )
    
    # Prepare output DataFrame
    output_df = filtered_df[['bigram', 'error_rate', 'error_mad', 'typing_time', 'time_mad', 
                           'instantaneous_error_rate', 'instantaneous_error_rate_mad', 
                           'totalCount']].copy()
    
    # Convert bigrams to uppercase for display
    output_df['bigram'] = output_df['bigram'].str.upper()
    
    # Save to CSV
    output_df.to_csv(output_path, index=False)
    print(f"Bigram instantaneous error rate data saved to {output_path}")
    
    return output_df

#-------------------------------------------------------------------------------
# Main
#-------------------------------------------------------------------------------
def verify_data_completeness(key_df, bigram_df):
    """
    Verify we have data for all expected keys and bigrams using the new definitions
    Parameters:
    key_df (DataFrame): DataFrame of individual key statistics
    bigram_df (DataFrame): DataFrame of bigram statistics
    """
    print("\n" + "="*50)
    print("data completeness verification")
    print("="*50)
    
    # Check individual keys
    all_left_home_keys = set(LEFT_HOME_KEYS)
    found_keys = set(key_df['key'])
    missing_keys = all_left_home_keys - found_keys
    
    print(f"\ntotal left home keys: {len(all_left_home_keys)}")
    print(f"keys with data: {len(found_keys)}")
    
    if missing_keys:
        print(f"missing keys: {', '.join(sorted(missing_keys)).upper()}")
    else:
        print("all left home block keys have data!")
    
    # Check bigrams (excluding same-key bigrams)
    all_possible_bigrams = set()
    for l1 in LEFT_HOME_KEYS:
        for l2 in LEFT_HOME_KEYS:
            # Skip same-key bigrams
            if l1 != l2:
                all_possible_bigrams.add(l1 + l2)
    
    found_bigrams = set(bigram_df['bigram'])
    missing_bigrams = all_possible_bigrams - found_bigrams
    
    print(f"\ntotal possible left home bigrams (excluding same-key bigrams): {len(all_possible_bigrams)}")
    print(f"bigrams with data: {len(found_bigrams)}")
    
    if len(missing_bigrams) > 10:
        print(f"missing {len(missing_bigrams)} bigrams, showing first 10: {', '.join(sorted(list(missing_bigrams)[:10])).upper()}")
    elif missing_bigrams:
        print(f"missing bigrams: {', '.join(sorted(missing_bigrams)).upper()}")
    else:
        print("all possible left home block bigrams have data!")
    
    # Verify that all bigrams are correctly structured (only left home keys)
    invalid_bigrams = []
    for bigram in found_bigrams:
        if len(bigram) != 2:
            invalid_bigrams.append(bigram)
        elif bigram[0] not in LEFT_HOME_KEYS or bigram[1] not in LEFT_HOME_KEYS:
            invalid_bigrams.append(bigram)
        elif bigram[0] == bigram[1]:  # Same-key bigram (should be excluded)
            invalid_bigrams.append(bigram)
            
    if invalid_bigrams:
        print(f"\nwarning: found {len(invalid_bigrams)} invalid bigrams: {', '.join(invalid_bigrams).upper()}")

def main():
    """Main function to execute the analysis pipeline"""

    frequency_cutoff = 1000000000

    # Get all CSV files in the directory
    csv_path = 'input/raws_nonProlific/*.csv' # 'input/raws_Prolific/*.csv'  
    letter_freq_path = 'input/frequency/english-letter-counts-google-ngrams.csv'
    bigram_freq_path = 'input/frequency/english-letter-pair-counts-google-ngrams.csv'

    # Load frequency data
    letter_frequencies, bigram_frequencies = load_frequency_data(letter_freq_path, bigram_freq_path)
    csv_files = glob.glob(csv_path)
    if not csv_files:
        print(f"No CSV files found at path: {csv_path}")
        print("Please check the path and ensure CSV files exist there.")
        return None
    print(f"Found {len(csv_files)} CSV files")
    all_data = []

    # Process each file
    for i, file_path in enumerate(csv_files):
        try:
            print(f"Reading file {i+1}/{len(csv_files)}: {os.path.basename(file_path)}", end="... ")
            df = pd.read_csv(file_path)
            print(f"loaded {len(df)} rows")
            if len(df) > 0:
                all_data.append(df)
            else:
                print(f"  Warning: File {os.path.basename(file_path)} contains no data")
        except Exception as e:
            print(f"Error reading file {os.path.basename(file_path)}: {str(e)}")
    if not all_data:
        print("No data was successfully loaded from any CSV file.")
        return None

    # Combine all data
    combined_data = pd.concat(all_data, ignore_index=True)
    print(f"Loaded {len(combined_data)} total typing data records")

    # Filter out intro-trial-1 data
    filtered_data = combined_data[combined_data['trialId'] != 'intro-trial-1'].copy()
    print(f"After filtering: {len(filtered_data)} records")

    # Process the data to calculate typing time
    processed_data = process_typing_data(filtered_data)

    # Analysis for 1-key and 2-key statistics
    left_home_key_stats, left_home_bigram_stats = analyze_key_stats(
        processed_data, keys_to_include=LEFT_HOME_KEYS
    )
    
    # Convert to DataFrames for analysis
    left_home_key_df = pd.DataFrame([stats for stats in left_home_key_stats.values()])
    left_home_bigram_df = pd.DataFrame([stats for stats in left_home_bigram_stats.values()])
    
    # Adjust metrics for frequency
    left_home_key_df = adjust_keys_for_frequency(left_home_key_df, letter_frequencies)
    left_home_bigram_df = adjust_bigrams_for_frequency(left_home_bigram_df, bigram_frequencies)

    # Filter bigrams by frequency threshold
    left_home_bigram_df = left_home_bigram_df[left_home_bigram_df['frequency'] >= frequency_cutoff].copy()

    # Analyze mirror pairs using the new key stats
    mirror_pair_df = analyze_mirror_pairs(left_home_key_stats, letter_frequencies)
    
    # Create visualizations
    visualize_mirror_pairs(mirror_pair_df)
    
    # Create 1-key visualizations
    visualize_left_home_key_error_rates(left_home_key_df)
    visualize_left_home_key_typing_times(left_home_key_df)
    visualize_left_home_key_error_vs_time(left_home_key_df)
    
    # Create 2-key (bigram) visualizations
    visualize_left_home_bigram_counts(left_home_bigram_df)
    visualize_left_home_bigram_error_rates(left_home_bigram_df)
    visualize_left_home_bigram_typing_times(left_home_bigram_df)
    visualize_left_home_bigram_error_vs_time(left_home_bigram_df)
    
    # Create comparison visualizations between 1-key and 2-key statistics
    visualize_key_vs_bigram_times(left_home_key_df, left_home_bigram_df)
    visualize_key_vs_bigram_errors(left_home_key_df, left_home_bigram_df)
    
    # Verify data completeness
    verify_data_completeness(left_home_key_df, left_home_bigram_df)
    
    # Save results to CSV files
    output_dir = 'output/'
    os.makedirs(output_dir, exist_ok=True)
    
    # Standard CSV outputs
    left_home_key_df.to_csv(f"{output_dir}left_home_key_analysis.csv", index=False)
    left_home_bigram_df.to_csv(f"{output_dir}left_home_bigram_analysis.csv", index=False)
    mirror_pair_df.to_csv(f"{output_dir}mirror_pair_analysis.csv", index=False)
    
    # Generate specialized output CSV files
    generate_accuracy_score_output_csv(left_home_key_df, mirror_pair_df, f"{output_dir}key_accuracy_statistics.csv")
    generate_speed_score_output_csv(left_home_key_df, mirror_pair_df, f"{output_dir}key_speed_statistics.csv")
    
    # Generate specialized bigram output CSV files
    generate_bigram_accuracy_output_csv(left_home_bigram_df, f"{output_dir}bigram_accuracy_statistics.csv")
    generate_bigram_speed_output_csv(left_home_bigram_df, f"{output_dir}bigram_speed_statistics.csv")

    # Generate composite bigram timing statistics
    generate_composite_bigram_speed_csv(left_home_bigram_df, left_home_key_df, f"{output_dir}bigram_composite_speed_statistics.csv")

    # Generate instantaneous error rate CSV files
    generate_instantaneous_error_rate_1key_csv(left_home_key_df, output_path=f"{output_dir}instantaneous_error_rate_1key.csv")
    generate_instantaneous_error_rate_2key_csv(left_home_bigram_df, output_path=f"{output_dir}instantaneous_error_rate_2key.csv")

    print(f"\nResults saved to {output_dir}")
    
    return {
        'left_home_key_stats': left_home_key_stats,
        'left_home_bigram_stats': left_home_bigram_stats,
        'mirror_pair_df': mirror_pair_df
    }

if __name__ == "__main__":
    results = main()